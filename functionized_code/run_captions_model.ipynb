{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains the following operations\n",
    "* Load the US Video data with Captions\n",
    "* Create iterators for train, validation and test datasets\n",
    "* Run the analysis with neural network models (including simple RNN, LSTM, CNN, and simple Linear NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_input as data_in\n",
    "import nnmodels as nnm\n",
    "import mlmodels as mlm\n",
    "import bow_models as bowm\n",
    "import train_eval\n",
    "import visualization as vis\n",
    "from torchtext import data\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "import sys\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Captions Data and Create Iterators for train, validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:\\Researching Data\\Youtube data\\caption_sector\\transcripts.txt' # should specify the directory for the captions\n",
    "path = r'D:\\Researching Data\\Youtube data\\caption_sector' # should specify the path to captions data\n",
    "TRAIN_VALID_TEST_R = (0.4, 0.4, 0.2)\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(1000000000)\n",
    "txt_list = []\n",
    "with open(data_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "    csv_reader = csv.reader(f, delimiter='\\n')\n",
    "    for row in csv_reader:\n",
    "        txt_list.append(', '.join(row))\n",
    "\n",
    "video_id = []\n",
    "txt_content = []\n",
    "for txt_row in txt_list:\n",
    "    video_id.append(txt_row[:11])\n",
    "    txt_content.append(txt_row[12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = pd.read_csv(r'D:\\Researching Data\\Youtube data\\USvideos.csv') # should specify the directory for US video data\n",
    "new_arr = fdata.drop_duplicates(\"video_id\", \"first\")[[\"video_id\", \"category_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_TEXT = txt_content\n",
    "new_id = video_id\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train, valid and test data are 1642 1642 822\n"
     ]
    }
   ],
   "source": [
    "train_indices, valid_indices, test_indices = data_in.split_train_test(len(new_TEXT), TRAIN_VALID_TEST_R)\n",
    "new_idtxt = pd.DataFrame(list(zip(new_id, new_TEXT)), columns=[\"video_id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pd = pd.merge(new_arr, new_idtxt, left_on=\"video_id\", right_on=\"video_id\")[[\"text\", \"category_id\"]]\n",
    "new_pd.loc[new_pd[\"category_id\"] != 25, \"category_id\"] = 0 # The category label can be replaced here\n",
    "new_pd.loc[new_pd[\"category_id\"] == 25, \"category_id\"] = 1 # The category label can be replaced here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline precision is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07355090112031173"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pd[new_pd[\"category_id\"] == 1].shape[0] / new_pd.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pd.iloc[train_indices].to_csv(path + \"\\\\train.csv\", header=None, index=None)\n",
    "new_pd.iloc[valid_indices].to_csv(path + \"\\\\valid.csv\", header=None, index=None)\n",
    "new_pd.iloc[test_indices].to_csv(path + \"\\\\test.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(\"text\", TEXT), (\"label\", LABEL)]\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                                            path = path,\n",
    "                                            train = 'train.csv',\n",
    "                                            validation = 'valid.csv',\n",
    "                                            test = 'test.csv',\n",
    "                                            format = 'csv',\n",
    "                                            fields = fields,\n",
    "                                            skip_header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data_in.build_iterator(BATCH_SIZE, device, train_data, valid_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Neural Network Models for Captions Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model_wordem = nnm.WordEmbAvg_2linear(INPUT_DIM, EMBEDDING_DIM, \n",
    "                                      HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "model_rnn = nnm.SimpleRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, \n",
    "                          OUTPUT_DIM, PAD_IDX)\n",
    "model_BLSTM = nnm.LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, \n",
    "                       N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "model_CNN = nnm.CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, \n",
    "                    OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "MODEL_DICT = {\"avg_embedding\": model_wordem, \"SimpleRNN\": model_rnn,\n",
    "              \"BLSTM\": model_BLSTM, \"CNN\": model_CNN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The result for Politics Category (category id = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently training the model:  avg_embedding\n",
      "Epoch 0: Dev Accuracy: 0.9316076452915485 Dev Loss:0.25120626791165424\n",
      "Epoch 1: Dev Accuracy: 0.9427620768547058 Dev Loss:0.18278173414560464\n",
      "Epoch 2: Dev Accuracy: 0.9440372884273529 Dev Loss:0.16400804112736994\n",
      "Epoch 3: Dev Accuracy: 0.9533155491718879 Dev Loss:0.18150587990665093\n",
      "Epoch 4: Dev Accuracy: 0.9499736153162442 Dev Loss:0.26239300222816664\n",
      "Test Loss: 0.159 | Test Acc: 95.98%\n",
      "Test Prec: nan% | Test Rec: 47.436%\n",
      "currently training the model:  SimpleRNN\n",
      "Epoch 0: Dev Accuracy: 0.9298047606761639 Dev Loss:0.2563474467740609\n",
      "Epoch 1: Dev Accuracy: 0.9298047606761639 Dev Loss:0.25429064809129787\n",
      "Epoch 2: Dev Accuracy: 0.9298047606761639 Dev Loss:0.2562699747773317\n",
      "Epoch 3: Dev Accuracy: 0.9298047606761639 Dev Loss:0.26208389851336295\n",
      "Epoch 4: Dev Accuracy: 0.9298047606761639 Dev Loss:0.2547185905277729\n",
      "Test Loss: 0.249 | Test Acc: 93.34%\n",
      "Test Prec: nan% | Test Rec: 0.000%\n",
      "currently training the model:  BLSTM\n",
      "Epoch 0: Dev Accuracy: 0.9298047606761639 Dev Loss:0.24390566005156591\n",
      "Epoch 1: Dev Accuracy: 0.9261989914453946 Dev Loss:0.23114759360368436\n",
      "Epoch 2: Dev Accuracy: 0.9175216922393212 Dev Loss:0.24604150337668565\n",
      "Epoch 3: Dev Accuracy: 0.9210541752668527 Dev Loss:0.2561538042500615\n",
      "Epoch 4: Dev Accuracy: 0.9294676368053143 Dev Loss:0.2599167349533393\n",
      "Test Loss: 0.241 | Test Acc: 93.34%\n",
      "Test Prec: nan% | Test Rec: 0.000%\n",
      "currently training the model:  CNN\n",
      "Epoch 0: Dev Accuracy: 0.7663871943950653 Dev Loss:0.5479716555430338\n",
      "Epoch 1: Dev Accuracy: 0.9177855299069331 Dev Loss:0.26510407560719895\n",
      "Epoch 2: Dev Accuracy: 0.9282657137283912 Dev Loss:0.24929070408241108\n",
      "Epoch 3: Dev Accuracy: 0.9340114914453946 Dev Loss:0.39583247636955304\n",
      "Epoch 4: Dev Accuracy: 0.9021605299069331 Dev Loss:0.2925615588632914\n",
      "Test Loss: 0.401 | Test Acc: 93.82%\n",
      "Test Prec: nan% | Test Rec: 6.538%\n"
     ]
    }
   ],
   "source": [
    "best_models, models_perf = train_eval.compare_models(MODEL_DICT, device, train_iterator, valid_iterator, test_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for Entertainment Category (category id = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently training the model:  avg_embedding\n",
      "Epoch 0: Dev Accuracy: 0.7518615149534665 Dev Loss:0.5224246829748154\n",
      "Epoch 1: Dev Accuracy: 0.7672226784320978 Dev Loss:0.482451863013781\n",
      "Epoch 2: Dev Accuracy: 0.7877579744045551 Dev Loss:0.4769950635158099\n",
      "Epoch 3: Dev Accuracy: 0.7789634145223178 Dev Loss:0.5109034180641174\n",
      "Epoch 4: Dev Accuracy: 0.7662699337189014 Dev Loss:0.6197512837556692\n",
      "Test Loss: 0.514 | Test Acc: 76.87%\n",
      "Test Prec: 61.598% | Test Rec: 37.170%\n",
      "currently training the model:  SimpleRNN\n",
      "Epoch 0: Dev Accuracy: 0.7209486399705594 Dev Loss:0.6013073577330663\n",
      "Epoch 1: Dev Accuracy: 0.7209486399705594 Dev Loss:0.6202571392059326\n",
      "Epoch 2: Dev Accuracy: 0.7239534476628671 Dev Loss:0.5893129373972232\n",
      "Epoch 3: Dev Accuracy: 0.7239534476628671 Dev Loss:0.5895780806358044\n",
      "Epoch 4: Dev Accuracy: 0.7245544092013285 Dev Loss:0.5946709009317251\n",
      "Test Loss: 0.596 | Test Acc: 72.34%\n",
      "Test Prec: nan% | Test Rec: 0.000%\n",
      "currently training the model:  BLSTM\n",
      "Epoch 0: Dev Accuracy: 0.7233524861244055 Dev Loss:0.5924635495130832\n",
      "Epoch 1: Dev Accuracy: 0.7022455437825277 Dev Loss:0.6014231351705698\n",
      "Epoch 2: Dev Accuracy: 0.7177532842526069 Dev Loss:0.5794070741304984\n",
      "Epoch 3: Dev Accuracy: 0.7118169550712292 Dev Loss:0.5884275665650001\n",
      "Epoch 4: Dev Accuracy: 0.7000615619696103 Dev Loss:0.6320956337910432\n",
      "Test Loss: 0.593 | Test Acc: 72.34%\n",
      "Test Prec: nan% | Test Rec: 0.000%\n",
      "currently training the model:  CNN\n",
      "Epoch 0: Dev Accuracy: 0.3947731004311488 Dev Loss:1.2230661603120656\n",
      "Epoch 1: Dev Accuracy: 0.7263572938167132 Dev Loss:1.0670029933636005\n",
      "Epoch 2: Dev Accuracy: 0.7407803707397901 Dev Loss:0.7194873575980847\n",
      "Epoch 3: Dev Accuracy: 0.7351078803722675 Dev Loss:0.6219306072363486\n",
      "Epoch 4: Dev Accuracy: 0.7271048312003796 Dev Loss:0.6232388569758489\n",
      "Test Loss: 0.730 | Test Acc: 73.54%\n",
      "Test Prec: nan% | Test Rec: 6.768%\n"
     ]
    }
   ],
   "source": [
    "best_models, models_perf = train_eval.compare_models(MODEL_DICT, device, train_iterator, valid_iterator, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36_main] *",
   "language": "python",
   "name": "conda-env-py36_main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
